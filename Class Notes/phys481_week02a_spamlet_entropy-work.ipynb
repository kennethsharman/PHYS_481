{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spamlet  entropy\n",
    "## PHYS 481 Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assignment examines the probability that random sources can generate meaningful outcomes.  Specifically, we consider how long it might take for a large number of monkeys with typewriters to eventually produce Hamlet.\n",
    "\n",
    "Several cases are considered\n",
    "1. independent distribution of letters assumed to be equally probable\n",
    "2. independent distribution of letters estimated from text\n",
    "2a. higher order distribution of multiple letters ie. pairs, triplets\n",
    "3. joint distribution of letters ie. aa, ab, ac .. zz\n",
    "4. independent distribution of words estimated from text\n",
    "5. joint distribution of words\n",
    "\n",
    "Probabilities and entropy are determined for each case.  The text of Hamlet from `gutenberg.org` is used to determine empirical frequencies.\n",
    "\n",
    "These tasks often involve calculations that often over/underflow even with double precision floating point variables.  This can be dealt with by using logarithms.\n",
    "\n",
    "Finally, random sequences were generated according to statistics for (in)dependent characters and words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load modules for math and plotting\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import urllib.request\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reading data\n",
    "\n",
    "We want to avoid pulling the file contents over the net every time that we run this cell.  This reduces uneccessary network traffic.  Perhaps more importantly, it means that we should always have a local copy of the data.  This will allow us to work even if we don't have network access, or if the original file is moved, changed, or deleted.\n",
    "\n",
    "Note: `urllib` will return byte data, so use the `decode` method to convert to a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "byte_data length: 204885\n",
      "char_data length: 202379\n"
     ]
    }
   ],
   "source": [
    "# Don't pull the same file every time!\n",
    "#\n",
    "\n",
    "local_filename = './phys481_hamlet.txt'\n",
    "remote_url = r'http://www.gutenberg.org/files/1524/1524-0.txt'\n",
    "\n",
    "# Could just get a local copy with browser or \"wget\" command.\n",
    "if not os.path.exists(local_filename):\n",
    "    byte_data = urllib.request.urlopen( remote_url ).read()    \n",
    "    with open(local_filename,'wb') as fid:\n",
    "        fid.write(byte_data)\n",
    "        \n",
    "with open(local_filename,'rb') as fid:\n",
    "    byte_data = fid.read()\n",
    "    \n",
    "print( 'byte_data length:',len(byte_data)) \n",
    "char_data = byte_data.decode() #'utf-8')\n",
    "print( 'char_data length:',len(char_data)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cleaning data\n",
    "\n",
    "The original file is formatted with line breaks for easy reading.  For this analysis we would like a simple sequence of characters without any format codes such as newline (`\\n`).  We do this by removing all non-ascii characters, and then converting everything to lower case.  Sequences with multple spaces are collapsed to single spaces.  This will allow us to obtain words by simply splitting on whitespace.\n",
    "\n",
    "Reality checks\n",
    "* There are 27 unique characters (26 letters plus space) \n",
    "* the contents of the word list seem reasonable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of characters:  185072\n",
      "unique characters:  27\n",
      "reality check- excerpt from char_list:  ['p', 'b', 'i', 't', 'h', 'y', 'd', 't', 'o', ' ', 't', 'r', 'i', 'e', 's', 't', 'l', 'o', 'g', ' ', ' ', 'y', ' ', 'u', ' ', 'n', ' ', 'i', 'e', ' ', 'd', 'g', 'r', ' ', 'e', ' ', ' ', 'a', 'd', ' ', 'i', 't', ' ', 'm', ' ', 'a', 't', 'i', 's', 'h', ' ', ' ', 'i']\n",
      "\n",
      "# of words:  35022\n",
      "unique words:  5224\n",
      "reality check- excerpt from word_list:  ['project', 'united', 'may', 'this', 'the', 'date', 'this', 'i', 'scene', 'castle', 'scene', 'the', 'ii', 'scene', 'in', 'prince', 'queen', 'friend', 'francisco', 'captain', 'platform', 'unfold', 'barnardo', 'and', 'good', 'horatio', 'marcellus', 'you', 'horatio', 'barnardo', 'of', 'watch', 'to', 'ears', 'and', 'from', 'the', 'the', 'it', 'it', 'together', 'i', 'thee', 'pale', 'this', 'horatio', 'so', 'marcellus', 'horatio', 'this', 'why', 'daily', 'task']\n"
     ]
    }
   ],
   "source": [
    "# collapse all whitespace (including newline)\n",
    "# to single spaces\n",
    "data = ' '.join(char_data.split())\n",
    "\n",
    "# eliminate all non-ascii characters (except space)\n",
    "# and convert to lower case\n",
    "valid_chars = string.ascii_letters + ' '\n",
    "char_list = [c.lower() for c in data if c in valid_chars]\n",
    "print( '# of characters: ', len(char_list) )\n",
    "print( 'unique characters: ', len(np.unique(char_list)) )\n",
    "print( 'reality check- excerpt from char_list: ', char_list[0:999:19] )\n",
    "\n",
    "# get words by joining with empty string and then splitting\n",
    "word_list = ''.join(char_list).split()\n",
    "print( '\\n# of words: ', len(word_list) )\n",
    "print( 'unique words: ', len(np.unique(word_list)) )\n",
    "print( 'reality check- excerpt from word_list: ',  word_list[0:999:19])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Factoring out repetition\n",
    "\n",
    "For the intial version of this report I wrote some code to build a catalog of single letters, then more code to work with pairs of letters, and yet more code to work with words and pairs of words.  All of this was essentially almost the same, so I pulled out the common operations into a functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_catalog_single( data, group=1 ):\n",
    "    ''' For a sequence (eg. list or string) of items,\n",
    "    step through and assemble a dict containing the\n",
    "    number of occurances of each item.\n",
    "    \n",
    "    group: default 1 to deal with individual items,\n",
    "    higher values to deal with groups of items ie.\n",
    "        data='abcd' , nitem=2\n",
    "        items='ab', 'bc', 'cd'\n",
    "    '''\n",
    "    \n",
    "    result = {}\n",
    "    for indx in range(len(data)-group):\n",
    "        item = ','.join( data[indx:indx+group] )\n",
    "        \n",
    "        if item not in result:\n",
    "            result[item] = 0\n",
    "        \n",
    "        result[item] += 1\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single letter catalog\n",
    "\n",
    "The most commonly occuring symbol is a space, followed by an `e` (as expected)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(' ', 35063), ('a', 11324), ('b', 2110), ('c', 3410), ('d', 5675), ('e', 17565), ('f', 3124), ('g', 2849), ('h', 9245), ('i', 10005), ('j', 200), ('k', 1415), ('l', 6862), ('m', 4644), ('n', 9739), ('o', 12856), ('p', 2461), ('q', 230), ('r', 9139), ('s', 9448), ('t', 14065), ('u', 4983), ('v', 1347), ('w', 3425), ('x', 206), ('y', 3554), ('z', 127)]\n"
     ]
    }
   ],
   "source": [
    "char_catalog_len1 = build_catalog_single( char_list, group=1)\n",
    "char_unique = sorted( char_catalog_len1.keys() )\n",
    "print([(c,char_catalog_len1[c]) for c in char_unique])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Letter pair catalog\n",
    "\n",
    "With 27 characters the largest number of posible pairs is 729.  The actual observed number is 505.  This is not surprising, as there are many combinations that are unlikely in English ie. 'zx', 'qq'.\n",
    "\n",
    "Looking at the most likely pairs, many of them appear to come from \"the\" ie. ' t', 'th', 'he', 'e '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique pairs:  505  max=27^2= 729\n",
      "[(' ,h', 2628), ('h,e', 3208), ('d, ', 3256), (' ,a', 3282), ('s, ', 4213), ('t,h', 4351), ('t, ', 4506), (' ,t', 4830), ('e, ', 6044)]\n"
     ]
    }
   ],
   "source": [
    "char_catalog_len2 = build_catalog_single( char_list, group=2)\n",
    "char_unique = np.array( sorted( char_catalog_len2.keys() ) )\n",
    "print('number of unique pairs: ', len(char_unique), ' max=27^2=',27*27)\n",
    "\n",
    "# sort from smallest to largest frequency\n",
    "argsort = np.argsort( [char_catalog_len2[c] for c in char_unique])\n",
    "print([(c,char_catalog_len2[c]) for c in char_unique[argsort[-9:]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single word catalog\n",
    "\n",
    "As expected, the most commonly occuring word is `the`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('in', 510), ('my', 516), ('i', 570), ('a', 611), ('you', 625), ('of', 800), ('to', 818), ('and', 1052), ('the', 1301)]\n"
     ]
    }
   ],
   "source": [
    "word_catalog_len1 = build_catalog_single( word_list, group=1)\n",
    "word_unique = np.array( sorted( word_catalog_len1.keys() ) )\n",
    "\n",
    "# sort from smallest to largest frequency\n",
    "argsort = np.argsort( [word_catalog_len1[c] for c in word_unique] )\n",
    "print([(c,word_catalog_len1[c]) for c in word_unique[argsort[-9:]]])\n",
    "#print([(c,word_catalog_len1[c]) for c in word_unique[0:999:19]])\n",
    "\n",
    "#plt.hist( [word_catalog_len1[c] for c in word_unique], bins=[1,2,3,4,5,6,7,8,9] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy of sequences\n",
    "\n",
    "A random sequence drawn independently from a set of 27 symbols should have an entropy of about 4.755 bits per symbol\n",
    "\n",
    "The spamlet text has an entropy of 4.10 bits per character, which means that spamlet is more predictable than pure randomness.\n",
    "Groups of two characters convey 7.46 bits per pair, which is roughly 0.5 bits less than the 8.2 bits expected from independent characters.  The entropy increases by smaller amounts with each additional character, so that 5 character groups have only about 1.5 bits per symbol more than 4 character groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_catalog_entropy( catalog ):\n",
    "    ''' calculate the entropy in bits for an occurrence histogram\n",
    "    '''\n",
    "    counts = [catalog[symbol] for symbol in catalog.keys()]  \n",
    "    prob = counts / np.sum(counts)\n",
    "    entropy = -np.sum( prob * np.log2(prob) )\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "character entropy: \n",
      " {1: 4.0952814218255424, 2: 7.458174869434918, 3: 10.098590230404971, 4: 12.11200082036454, 5: 13.678302669204667}\n",
      "\n",
      " incremental entropy\n",
      "3.362893447609376\n",
      "2.640415360970053\n",
      "2.0134105899595696\n",
      "1.5663018488401264\n"
     ]
    }
   ],
   "source": [
    "char_entropy = {}\n",
    "for n in [1,2,3,4,5]:\n",
    "    catalog = build_catalog_single( char_list, n)\n",
    "    char_entropy[n] = single_catalog_entropy(catalog)\n",
    "print('character entropy: \\n', char_entropy)\n",
    "\n",
    "print('\\n incremental entropy')\n",
    "for n in [2,3,4,5]:\n",
    "    print(char_entropy[n] - char_entropy[n-1])\n",
    "\n",
    "#print('\\n', -1.0/27.0 * np.log2(1/27.0) * 27)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word sequences\n",
    "\n",
    "The entropy per word is roughly 9.4 bits, which is only slightly less than 10.1 bits per 3 characters. Adding a second word provides about 3.6 bits, while a third word gives less than 1 bit per symbol. This means that if we already know three words in a sequence then we can predict the next word with better than 50% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "word entropy: \n",
      " {1: 9.401312892978655, 2: 13.995995066223381, 3: 14.921734766322679}\n"
     ]
    }
   ],
   "source": [
    "word_entropy = {}\n",
    "for n in [1,2,3]:\n",
    "    catalog = build_catalog_single( word_list, n)\n",
    "    word_entropy[n] = single_catalog_entropy(catalog)\n",
    "print('\\nword entropy: \\n', word_entropy)\n",
    "#print(nentropy[1:] - nentropy[0:-1] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joint probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_catalog_double( data ):\n",
    "    ''' for each symbol (ie. character or word), \n",
    "    determine the distribution of the next symbol\n",
    "    '''\n",
    "    result = {}\n",
    "    for indx in range(len(data)-2):\n",
    "        item1, item2 = data[indx:indx+2] \n",
    "        \n",
    "        # use the empty string to count level1\n",
    "        if item1 not in result:\n",
    "            result[item1] = {'':0}\n",
    "        \n",
    "        result[item1][''] += 1\n",
    "        \n",
    "        if item2 not in result[item1]:\n",
    "            result[item1][item2] = 0\n",
    "            \n",
    "        result[item1][item2] += 1            \n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('', 818), ('a', 18), ('abide', 1), ('act', 2), ('all', 2), ('and', 2), ('any', 4), ('anyone', 1), ('are', 1), ('arraign', 1), ('be', 36), ('bear', 3), ('beard', 1), ('bed', 4), ('beg', 1), ('begin', 1), ('beguile', 1), ('believe', 1), ('blame', 3)]\n"
     ]
    }
   ],
   "source": [
    "word_catalog = build_catalog_double( word_list)\n",
    "word_unique = sorted( word_catalog['to'].keys() )\n",
    "print([(c,word_catalog['to'][c]) for c in word_unique[0:19]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability_single( data, catalog=None, group=1 ):\n",
    "    ''' Walk through a sequence of symbols and determine the total probability\n",
    "    for independent outcomes by multiplying individual probabilities\n",
    "    '''\n",
    "    \n",
    "    if catalog is None:\n",
    "        catalog = build_catalog_single(data, group = group)\n",
    "    \n",
    "    symbol_unique = sorted( list(catalog.keys()) )\n",
    "    counts = np.array( [catalog[w] for w in symbol_unique] )\n",
    "    probs = counts / np.sum(counts)\n",
    "    logprobs = np.log2(probs)\n",
    "    \n",
    "    logprob = 0.0\n",
    "    for indx in range(len(data)-group):\n",
    "        symbol= ','.join(data[indx:indx+group])\n",
    "        #if symbol not in symbol_unique:\n",
    "        #    print('boom: ', indx, nitem, symbol, data[indx:indx+nitem])\n",
    "        #    continue\n",
    "        logprob += logprobs[ symbol_unique.index(symbol) ]\n",
    "        \n",
    "    return logprob / group\n",
    "\n",
    "#probability_single(char_list, nitem=3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### independent (grouped)  probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "independent char, n=1 -757917.8280183714\n",
      "independent char, n=2 -690142.2115430712\n",
      "independent char, n=3 -622978.6651168145\n",
      "independent word, n=1 -329243.37882501545\n",
      "independent word, n=2 -245069.87360965964\n"
     ]
    }
   ],
   "source": [
    "logprob = {}\n",
    "for n in [1,2,3]:\n",
    "    name = 'independent char, n={}'.format(n)\n",
    "    logprob[name] = probability_single(char_list, group=n) \n",
    "    print(name, logprob[name])\n",
    "    \n",
    "for n in [1,2]:\n",
    "    name = 'independent word, n={}'.format(n)\n",
    "    logprob[name] = probability_single(word_list, group=n)     \n",
    "    print(name, logprob[name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joint probability\n",
    "\n",
    "Start with the probability of getting the first character\n",
    "\n",
    "$$ p(c_1) $$\n",
    "\n",
    "next, use the probability of getting the second character following the first\n",
    "\n",
    "$$ p(c_2 | c_1 )$$\n",
    "\n",
    "and so on\n",
    "\n",
    "$$ p = p(c_1) \\,\\times\\, p(c_2 | c_1) \\,\\times\\, \\ldots p(c_n, c_{-1}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability_double( data, catalog=None):\n",
    "    \n",
    "    if catalog is None:\n",
    "        catalog = build_catalog_double(data)\n",
    "    \n",
    "    #symbol_unique = sorted( [w for w in catalog.keys() ] )\n",
    "    #counts = np.array( [catalog[w] for w in symbol_unique] )\n",
    "    #probs = counts / np.sum(counts)\n",
    "    #logprobs = np.log2(probs)\n",
    "    \n",
    "    symbol1 = data[0]\n",
    "    logprob = np.log2( catalog[symbol1][''] / len(data) ).astype(np.float64)\n",
    "    \n",
    "    for symbol2 in data[1:]:\n",
    "        cat = catalog[symbol1]\n",
    "        logprob += np.log2( cat[symbol2] / cat[''] )\n",
    "        symbol1 = symbol2\n",
    "        \n",
    "    return logprob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'independent char, n=1': -757917.8280183714, 'independent char, n=2': -690142.2115430712, 'independent char, n=3': -622978.6651168145, 'independent word, n=1': -329243.37882501545, 'independent word, n=2': -245069.87360965964, 'joint char': -622383.5436114544, 'joint word': -160919.94903523603}\n"
     ]
    }
   ],
   "source": [
    "logprob['joint char'] = probability_double(char_list)\n",
    "logprob['joint word'] = probability_double(word_list)\n",
    "print(logprob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using joint statistics increases the likelihood of getting an outcome by more than adding a 3rd character.  Similarly, with joint word statistics we find the outcome is hugely more likely than considering pairs of words as being purely independent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['independent char, n=1', 'independent char, n=2', 'independent char, n=3', 'independent word, n=1', 'independent word, n=2', 'joint char', 'joint word']\n",
      "     -160919.95 joint word\n",
      "     -245069.87 independent word, n=2\n",
      "     -329243.38 independent word, n=1\n",
      "     -622383.54 joint char\n",
      "     -622978.67 independent char, n=3\n",
      "     -690142.21 independent char, n=2\n",
      "     -757917.83 independent char, n=1\n"
     ]
    }
   ],
   "source": [
    "namelist = list(logprob.keys()) ; print(namelist)\n",
    "argsort = np.argsort( [logprob[name] for name in namelist])\n",
    "linelist = ['{:15.8} {}'.format(logprob[name], name) for name in namelist]\n",
    "print('\\n'.join( sorted( linelist ) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fake spam\n",
    "\n",
    "We can assemble sequences of characters or words that are randomly drawn from observed distributions.  Results for independent characters are gibberish, and adding inter-character relationships doesn't help much.\n",
    "\n",
    "Sequences of independent randomly selected words have a superficial resemblance to English, but on closer examination make no sense.\n",
    "\n",
    "Random sequences of related words almost sound meaningful when read aloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fakespam( data, size=99, independent=False):\n",
    "    \n",
    "    catalog = build_catalog_double(data)\n",
    "    \n",
    "    # level1 statistics\n",
    "    items = [i for i in catalog.keys() ]\n",
    "    counts = np.array( [catalog[i][''] for i in items] )\n",
    "    probs = counts / np.sum(counts)    \n",
    "    \n",
    "    if independent:\n",
    "        fakespam = np.random.choice(items, size=99, p=probs )\n",
    "        return fakespam\n",
    "    \n",
    "    fakespam = []\n",
    "    fakespam.append( np.random.choice(items, size=1, p=probs )[0] )  #first item\n",
    "    for n in range(size):\n",
    "        previous_item = catalog[fakespam[-1]]\n",
    "        next_itemlist = [i for i in previous_item.keys() if i != '']\n",
    "        counts = [previous_item[i] for i in next_itemlist]\n",
    "        probs = np.array(counts) / np.sum(counts)\n",
    "        fakespam.append( np.random.choice(next_itemlist, size=1, p=probs )[0] )\n",
    "        \n",
    "    return fakespam    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random characters\n",
      " lh osuencuiiahsga esdbwouw sih  t  a  rs oasniotimeu  sea orr  doo  leoytgtr  eopeoeb dit is  teo   \n",
      "\n",
      "related characters\n",
      "  ndas mar u jele nd dere inu bulliteye f r andone hechefll bomewindeat tmblindooy elel amice chamy theinth plas iny hare ama aser thishedellle imso ton amll s a fores w thamy he oty monthio ply yed gioffe ss se k z dinsus mator vet myo f s s whabereas astheg icr thiki n slmyos ar dedint fui be he sr \n",
      "\n",
      "random words: \n",
      " you sure and constantly you of lets up have your i the rapier heart him us that tonight element son perchance designd so crowns king poison but for again agree set him every own not feeds my my gives and have action marcellus sends hamlet but to displaying servd o walk donations i no must offence highest what his not lords but are by worm than polonius bet o note of horatio door may you absurd guilty land of know why do drooping rosencrantz beard this and is for trumpet my and my should such to that well takes \n",
      "\n",
      "related words: \n",
      " you shall unfold hamlet ay my noble mind dislike anything we here was no my lord your love methought it horatio my lord enter queen sleep so rude against the imminent be short the consonancy of the salt burn out his face doth all after your skill both sides i have grounds more engagd help to the cannons to heaven ill court it not charge led by the earth to sound what i could nothing there oertook ins time forth polonius wherefore should scape detecting i know from all creatures sitting at gravemaking horatio now or use no fairy takes\n"
     ]
    }
   ],
   "source": [
    "print('random characters\\n', ''.join( fakespam(char_list, independent=True, size=299)), '\\n' )        \n",
    "print('related characters\\n', ''.join( fakespam(char_list, independent=False, size=299)), '\\n' )    \n",
    "print('random words: \\n',' '.join( fakespam(word_list, independent=True)), '\\n' ) \n",
    "print('related words: \\n', ' '.join( fakespam(word_list, independent=False)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Waiting for \"To be or not to be\"\n",
    "\n",
    "    probability per monkey * number of monkeys * number of keys per monkey\n",
    "\n",
    "Let us consider only the classic fragment \"to be or not to be'.  The probability of getting this outcome for related key strokes is roughly 2e-19, which is much much much larger than most of the numbers which we have encountered recently.  This means that the desired output is likely to occur if 6 billion typeists were each hitting 1 key per second for 25 years.\n",
    "\n",
    "Working with related words drastically increases the probability to 6e-10, which would be likely after only 30 minutes with a million monkeys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "249.67256377502338\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-160670.276471461"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_change = logprob['joint word']  - logprob['independent char, n=1'] \n",
    "\n",
    "mass = {'population':6e9, 'earth':5.972e24, 'sun':1.989e30, 'galaxy':6e42, 'universe':6.0e51}\n",
    "time = {'day':86400, 'universe':4.1e17} \n",
    "\n",
    "print (np.log(mass['universe']/9.11e-31) + np.log(time['universe']*1e9 ) )\n",
    "logprob['joint word'] + np.log(mass['universe']/9.11e-31) + np.log(time['universe']*1e9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2233646883344667e-19\n",
      "6.071506486153084e-10\n",
      "1.0517404321697361\n",
      "1.4571615566767402\n"
     ]
    }
   ],
   "source": [
    "char_logprob = probability_double('to be or not to be' , catalog=build_catalog_double(char_list)) \n",
    "word_logprob = probability_double('to be or not to be'.split() , catalog=build_catalog_double(word_list)) \n",
    "\n",
    "print( np.exp(char_logprob))\n",
    "print( np.exp(word_logprob))\n",
    "\n",
    "print( np.exp(char_logprob)  * mass['population'] * time['day']*365*25 )\n",
    "print( np.exp(word_logprob)  * 1e6 * (60*40) )\n",
    "\n",
    "#print( np.exp(char_logprob + np.log(mass['earth']) + np.log(time['day']*365*1e6 )))\n",
    "#print( np.exp(word_logprob + np.log(mass['earth']) + np.log(time['day'] )))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Don't hold your breath\n",
    "\n",
    "Imagine that the entire mass of our universe was converted to electrons and each electron could somehow hit one key per nanosecond for the entire age of the universe.  This would give a very large number of attempts\n",
    "\n",
    " $$\\exp(250) $$\n",
    " \n",
    "but this is overwhelmed by the miniscule log-probability of order -160000. \n",
    "\n",
    "Conclusion: Shakespeare got lucky."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "A finite number of monkeys might eventually be able to generate a small part of Hamlet's soliloquy, but producing the entire play through random chance is effectively impossible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question \\#1: what is the entropy of \"simplified Hamlet\" (Spamlet)?\n",
    "    \n",
    "### Question \\#2: what is the probability that a monkey with a uniform random selection of 27-keys would produce Spamlet?  In other words, how many different sequences with 167774 characters are there?\n",
    "\n",
    "### Question \\#3: how does the probability change if the chance of hitting any given key was not 1/27, but the same as the distribution of Spamlet?\n",
    "\n",
    "### Question \\#4: determine the joint probability of each 2-key sequence eg. 'aa', 'ab', 'ac' from Spamlet.  How does the probability of producing Spamlet change if the monkey hits keys according to this distribution?\n",
    "\n",
    "### Question \\#5: calculate the entropy of  2-key sequences, and entire words for Spamlet.\n",
    "\n",
    "### Optional Question \\#6: write a program to generate sequences of text that sound somewhat like Shakespeare.  See for inspiration http://www.elsewhere.org/journal/pomo/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grading rubric\n",
    "\n",
    "I generally mark assignments by starting at 10/10 and subtracting with a resolution of 1/2.  \n",
    "\n",
    "* introduction/overview: what are we trying to do and why?\n",
    "    * completely missing -1\n",
    "    * inadequate/problematic -1/2 (eg. In this assignment I answered the questions)\n",
    "    \n",
    "* conclusions/summary: what did we do (could be in intro)?  how does it fit together?\n",
    "    * completely missing -1\n",
    "    * inadequate/problematic -1/2\n",
    "    \n",
    "* graphing problems: axes labels, titles, legends, not horribly cluttered, etc.\n",
    "    * one minor -1/2\n",
    "    * multiple -1\n",
    "    \n",
    "* interstitial text: the average page could be mostly code and output, but some bridging discussion is required\n",
    "    * inadequate -1/2    \n",
    "    * essentially none -1\n",
    "    \n",
    "* function docstring (can sometimes skip for utterly trivial function)\n",
    "    * missing one -1/2\n",
    "    * missing many -1\n",
    "    \n",
    "* duplicated code: should be factored into common functions\n",
    "    * several repeated blocks of multiple lines -1/2\n",
    "    * no attempt to remove many redundancies -1\n",
    "    \n",
    "For this assignment there should be numerical values for\n",
    "    1. single symbol entropy\n",
    "    2. uniform probability (1/27)^n\n",
    "    3. empirical probability (1/n1)^n1 * (1/n2)^n2 ...\n",
    "    4. empirical probability & entropy for pairs of characters \n",
    "    5. entropy for words\n",
    "These may not be exactly the same as my results, depending on how they trimmed the text.\n",
    "I also did many other calculations that aren't required.\n",
    "\n",
    "* numerical/calculation/conceptual errors\n",
    "    * one minor -1/2\n",
    "    * multiple minor or one major -1\n",
    "\n",
    "    \n",
    "Either one of question 5 or 6 is acceptable.    \n",
    "    \n",
    "Reality check: make a note if the assignment is basically ok, but the final score is 5 or less.\n",
    "Use your judgement and let me know if you are seeing common patterns. \n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
